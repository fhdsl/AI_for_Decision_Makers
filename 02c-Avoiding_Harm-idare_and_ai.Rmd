


```{r, include = FALSE}
ottrpal::set_knitr_image_path()
```

# IDARE and AI

IDARE stands for Inclusion, Diversity, Anti-Racism, and Equity. It is an acronym used by some institutions (such as the [Johns Hopkins Bloomberg School of Public Health](https://publichealth.jhu.edu/offices-and-services/office-of-inclusion-diversity-anti-racism-and-equity-idare), [the University of California, Davis](https://health.ucdavis.edu/diversity-inclusion/committees/departmental-committees-index.html), and the [University of Pensylvania Perlemen School off Medicine](https://www.med.upenn.edu/neurology/idare/)) to remind people about practices to improve [social justice](https://en.wikipedia.org/wiki/Social_justice). As we strive to use AI responsibly, keeping the major principles of IDARE in mind will be helpful to better ensure that individuals of all backgrounds and life experiences more equally benefit from advances in technological and that technology is not used to perpetuate harm.  

## AI is biased

Humans are biased, therefore data from text written by humans is often also biased, which mean AI systems built on human text are trained to be biased, even those created with the best intentions (@pethig_biased_2023).

To better understand your own personal bias, consider taking a test at https://implicit.harvard.edu/. 

It is nearly impossible to create a training dataset that is free from all possible bias and include all possible example data, so by necessity the data used to train AI systems are generally biased in some way and lack data about people across the full spectrum of backgrounds and life experiences. This can lead to AI-created products that cause discrimination, abuse, or neglect for certain groups of people, such as those with certain ethnic or cultural backgrounds, genders, ages, sexuality, capabilities, religions or other group affiliations. Our goal is to create and use AI systems that are as inclusive and unbiased as possible while also keeping in mind that the system is not perfect. 

## Examples of AI Bias

There are many examples in which biased AI systems were used in a context with negative consequences. 

### Amazon's resume system was biased against women

Amazon used an AI system was to help filter candidates for jobs. They started using the system in 2014. In 2015, it was discovered that the system penalized resumes that included words like "women's", and also for graduates of two all-women's colleges (@dastin_insight_2018). 

How did this happen? The model was trained on resume's of existing Amazon employees and most of their employees were male. Thus the training data for this system was not gender inclusive, which lead to bias in the model.

###

## Mitigation

When working with AI systems, users should actively identify any potential biases used in the training data for a particular AI system. In particular, the user should look for harmful data, such as discriminatory and false associations included in the training dataset, as well as verify whether the training data is adequately inclusive for your needs. A lack of data about certain ethnic or gender groups or disabled individuals could result in a product that does not adequately consider these groups, ignores them all together, or makes false associations. Where possible, users of commercial AI tools should ask prompts in a manner that includes concern for equity and inclusion, they should use tools that are transparent about what training data was used and limitations of this data, and they should always question the responses from the tool for possible bias. 



:::{.query}
Why did you assume that the doctor was male?
:::


Those developing or augmenting models should also evaluate the training data and the model for biases and false associations as it is being developed instead of waiting to test the product after creation is finished. This includes verifying that the product works properly for potential use cases from a variety of ethnic, gender, ability, socioeconomic, language, and educational backgrounds. When possible, the user should also augment the training dataset with data from groups that are missing or underrepresented in the original training dataset. 

# Don't use AI for Human Decisions

There is a common misconception that AI tools might make better decisions for humans because they are believed to not be biased like humans. However since they are built by humans and trained on human data, they are also baised. It is possible that in the future AI systems specifically trained to avoid bias, to be inclusive, to be anti-racist,  for specific contexts may be helpful to enable a more neutral party, but that is not currently possible.

AI should never be used to make or help make employment decisions about applicants or employees. This includes recruitment, hiring, retention, promotions, transfers, performance monitoring, discipline, demotion, terminations, or other decisions. Furthermore, it is vital that teams hired for the development, auditing or testing of AI tools be as inclusive as possible and should follow the current standards for DEI in hiring other Fred Hutch positions. This will help to ensure that different perspectives and concerns are considered. 

 

Resources:  

https://arxiv.org/abs/2311.14096 

https://www3.weforum.org/docs/WEF_A_Blueprint_for_Equity_and_Inclusion_in_Artificial_Intelligence_2022.pdf  

https://magazine.jhsph.edu/2022/how-biased-data-and-algorithms-can-harm-health  

https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-standard/ 

https://www.tandfonline.com/doi/full/10.1080/08839514.2023.2176618  