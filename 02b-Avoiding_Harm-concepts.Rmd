


```{r, include = FALSE}
ottrpal::set_knitr_image_path()
```



## Societal Impact

In this chapter we will discuss the some of the major ethical considerations in terms of possible societal consequences for the use or development of AI tools:

1) **Inadvertent Harm** - Data and technology intended to serve one purpose may be reused by others for unintended purposes.
1) **Replacing Humans** - AI tools can help humans, but they are not a replacement. Humans are still much better at generalizing their knowledge to other contexts.
1) **Inappropriate Uses** - There are situations in which using AI might not be appropriate now or in the future, in which as a society we may decide humans should always be involved.
1) **Bias** - AI models are built on data and code that were created by biased humans, thus bias can be further perpetuated.
1) **Security or Privacy Issues** - Uploading, pasting or typing in proprietary or private data, code, text, images or other files into commercial generative AI tools may be leaked not only to the developers of the commercial tool, but potentially also to other users


Note that this is an incomplete list; additional ethical concerns will become apparent as we continue to use these new technologies. We highly suggest that users of these tools **careful to learn more about the specific tools they are interested in** and to be **transparent** about the use of these tools, so that as new ethical issues emerge, we will be better prepared to understand the implications.

## Inadvertent Harm 

Of course using AI to help you perform a harmful action would result in intentional harm. This may sound like an obvious and easy issue to avoid, at least by those with good intent. However, the consequences may be much further reaching than might be first anticipated.

Perhaps you or your company develop an AI tool that helps to identify individuals that might especially benefit from a product or service that you offer. This in and of itself is likely not harmful. However, the data you have used, the data that you may have collected, and the tool that you have created, all could be used for other malicious reasons, such as targeting specific groups of people for advertisements when they are vulnerable.

Therefore it is critical that we be considerate of the downstream consequences of what we create and what might happen if that technology or data was used for other purposes. 

### Tips for avoiding inadvertent harm

* Be careful about what content you share publicly, as it could be used for malicious purposes.
* Consider how the content, data, or newly developed AI tool might be used by others.
* Ask the AI tools to help you, but do not rely on them alone.


## Replacing Humans

While AI systems are useful, they **do not replace the strengths that humans have** for innovating new ideas or methods, for  evaluating how the content generated by AI integrates into the larger picture of a project, or in evaluating the downstream consequences of the content. 

Computer science is a field that has historically lacked diversity. It is critical that we support diverse new learners of computer science, as we will continue to need human involvement in the development and use of AI tools. This can help to ensure that more diverse perspectives are accounted for in our understanding of how these tools should be used responsibly.

### Tips for supporting human contributions

* Avoid thinking that content by AI tools must be better than that created by humans, as this is not true. 
* Recall that humans wrote the code to create these AI tools and that the data used to train these AI tools also came from humans. Many of the large commercial AI tools were trained on websites and other content from the internet.
* Be transparent where possible about when you do or do not use AI tools, give credit to the humans involved as much as possible.

:::{.ethics}
A new term in the medical field called [AI paternalism](https://www.technologyreview.com/2023/04/21/1071921/ai-is-infiltrating-health-care-we-shouldnt-let-it-make-decisions/) describes the concept that doctors (and others) may trust AI over their own judgment or the experiences of the patients they treat. This has already been shown to be a problem with earlier AI systems intended to help distinguish patient groups. Not all humans will necessarily fit the expectations of the AI model if it is not very good at predicting edge cases [@AI_paternalism]. Therefore, in all fields it is important for us to not forget our value as humans in our understanding of the world. 
:::

## Inappropriate Uses

There are situations in which we may, as a society, not want an automated response. There may even be situations in which we do not want to bias our own human judgment by that of an AI system. There may be other situations where the efficiency of AI may also be considered inappropriate. While many of these topics are still under debate and AI technology continues to improve, we challenge the readers to consider such cases given what is currently possible and what may be possible in the future. 

Some reasons why AI may not be appropriate for certain situation include:

- Despite the common misconception that AI systems have clearer judgment than humans, they are in fact typically just as prone to bias and sometimes even exacerbate bias (@pethig_biased_2023).  There are some very mindful researchers working on these issues in specific contexts and making progress where AI may actually improve on human judgment, but generally speaking AI systems are currently typically biased and reflective of human judgment but in a more limited manner based on the context in which they have been trained.
- AI systems can behave in unexpected ways (@gichoya_ai_2022). 
- Humans are still better than AI at generalizing what they learn for new contexts.
- Humans can better understand the consequences of discussions from a humanity standpoint. 

Some examples where it may be considered inappropriate for AI systems to be used include:

- In the justice system to determine if someone is guilty of a crime or to determine the punishment of someone found guilty of a crime.
- It may be considered inappropriate for AI systems to be used in certain warfare circumstances.


### Tips for avoiding inappropriate uses

* Stay up-to-date on current practices and standards for your field, as well as up-to-date on the news for how others have experienced their use of AI.
* Stay involved in discussions about appropriate uses for AI, particularly for policy.
* Begin using AI slowly and iteratively to allow time to determine the appropriateness of the use. Some issues will only be discovered after some experience.
* Involve a diverse group of individuals in discussions of intended uses to better account for a variety of perspectives.
* Seek outside expert opinion whenever you are unsure about your AI use plans.
* Consider AI alternatives if something doesn't feel right.

## Bias 

One of the biggest concerns is the potential for AI to further perpetuate bias. AI systems are trained on data created by humans. If this data used to train the system is biased (and this includes existing code that may be written in a biased manner), the resulting content from the AI tools could also be biased. This could lead to discrimination, abuse, or neglect for certain groups of people, such as those with certain ethnic or cultural backgrounds, genders, ages, sexuality, capabilities, religions or other group affiliations. 

It is well known that data and code are often biased [@belenguer_ai_2022]. The resulting output of AI tools should be evaluated for bias and modified where needed. Please be aware that because bias is intrinsic, it may be difficult to identify issues. Therefore, people with specialized training to recognize bias should be consulted. It is also vital that evaluations be made throughout the software development process of new AI tools to check for and consider potential perpetuation of bias.

### Tips for avoiding bias

* Be aware of the biases in the data that is used to train AI systems. 
* Check for possible biases within data used to train new AI tools.
  - Are there harmful data values? Examples could include discriminatory and false associations.
  - Are the data adequately inclusive? Examples could include a lack of data about certain ethnic or gender groups or disabled individuals, which could result in code that does not adequately consider these groups, ignores them all together, or makes false associations.
  - Are the data of high enough quality?  Examples could include data that is false about certain individuals. 
* Evaluate the code for new AI tools for biases as it is developed. Check if any of the criteria for weighting certain data values over others are rooted in bias.
* Consider the possible outcomes of the use of content created by AI tools. Consider if the content could possibly be used in a manner that will result in discrimination.

See @belenguer_ai_2022 for more guidance. We also encourage you to check out the following video for a classic example of bias in AI:

```{r, fig.align="center", fig.alt = "video", echo=FALSE, out.width="90%"}
knitr::include_url("https://www.youtube.com/embed/TWWsW1w-BVo?si=YLGbpVKrUz5b56vM")
```


For further details check out this [course](https://www.coursera.org/learn/algorithmic-fairness) on Coursera about building fair algorithms. We will also describe more in the next section.


## Security and Privacy issues

Commercial AI tools are often not designed to protect users from unknowingly submitting prompts that include propriety are private information. Different AI tools have different practices in terms of how they do or do not collect data about the prompts that people submit. They also have different practices in terms of if they reuse information from prompts to other users. Thus if users submit prompts that include propriety or private information, they run the risk of that information being viewable not only by the developers/maintainers of the AI tool used, but also by other users who use that same AI tool. Note that the AI system itself may not be trained on responses for how prompt data is collected or not. 

Furthermore, AI tools are not always trained in a way that is particularly conscious of data security. If for example, code is written using these tools by users who are less familiar with coding security concerns, protected data or important passwords may be leaked within the code itself. AI systems may also utilize data that was actually intended to be private. 

It is also important to consider what data your the responses that you get from an AI tool might actually be using.


### Tips for reducing security and privacy issues

* Check that no sensitive data, such as Personal Identifiable Information (PII) or propriety information becomes public through prompts to commercial AI systems.
* Consider purchasing a license for a private AI system if needed or create your own if you wish to work with sensitive data (seek expert guidance to determine if the AI systems are secure enough).
* Promote for regulation of AI tools by voting for standards where possible.
* Ask AI tools for help with security when using commercial tools, but to not rely on them alone. In some cases, commercial AI tools will even provide little guidance about who developed the tool and what data it was trained on, regardless of what happens to the prompts and if they are collected and maintained in a secure way.
* Consult with an expert about data security if you want to design or use a AI tool that will regularly use private or propriety data.

<div class = "query">
Are there any possible data security or privacy issues associated with the plan you proposed?
</div>


## Summary

Here is a summary of all the tips we suggested:

:::{.ethics}

* Disclose when you use AI tools to create content.
* Be aware that AI systems are biased and their responses are likely biased. Any content generated by an AI system should be evaluated for potential bias.
* Be aware that AI systems may behave in unexpected ways. Implement new AI solutions slowly to account for the unexpected. Test those systems and try to better understand how they work in different contexts.
* Be aware that humans are still better at generalizing concepts to other contexts.
* Carefully consider if an AI solution is appropriate for your context.
* Credit human authors by citing them and adhering to copyright restrictions.
* Ensure that prompts to commercial tools don't include private or propriety data or information.
* Cross-check content from AI tools by using multiple AI tools - but check that each tool meets the privacy and security restrictions that you need.
* Don't assume AI-generated content is real, accurate, consistent, current, or better than that of a human.
* Ask the AI tools to help you understand:
  * Sources for the content that you can cite
  * Any decision processes in how the content was created 
  * Potential limitations 
  * Potential security or privacy issues
  * Potential downstream consequences of the use of the content
* Always have expert humans review the content and value your own contributions and thoughts.
* Emphasize training and education about AI and recognize that best practices will evolve as the technology evolves.

:::

Overall, we hope that these guidelines and tips will help us all use AI tools more responsibly. We recognize however, that as this is emerging technology and more ethical issues will emerge as we continue to use these tools in new ways. AI tools can even help us to use them more responsibly when we ask the right additional questions, but remember that human review is always necessary. Staying up-to-date on the current ethical considerations will also help us all continue to use AI responsibly.





