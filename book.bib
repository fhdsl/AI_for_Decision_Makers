@Manual{rmarkdown2021,
  title = {rmarkdown: Dynamic Documents for R},
  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
  year = {2021},
  note = {R package version 2.10},
  url = {https://github.com/rstudio/rmarkdown},
}

@Book{Xie2018,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  note = {ISBN 9781138359338},
  url = {https://bookdown.org/yihui/rmarkdown},
}

@Book{Xie2020,
  title = {R Markdown Cookbook},
  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2020},
  note = {ISBN 9780367563837},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook},
}

@ARTICLE{Mattson2014,
  	AUTHOR={Mattson, Mark P.},   
	 TITLE={Superior pattern processing is the essence of the evolved human brain},      
	JOURNAL={Frontiers in Neuroscience},      
	VOLUME={8},           
	YEAR={2014},      
	  URL={https://www.frontiersin.org/articles/10.3389/fnins.2014.00265},       
	DOI={10.3389/fnins.2014.00265},      
	ISSN={1662-453X},   
	ABSTRACT={Humans have long pondered the nature of their mind/brain and, particularly why its capacities for reasoning, communication and abstract thought are far superior to other species, including closely related anthropoids. This article considers superior pattern processing (SPP) as the fundamental basis of most, if not all, unique features of the human brain including intelligence, language, imagination, invention, and the belief in imaginary entities such as ghosts and gods. SPP involves the electrochemical, neuronal network-based, encoding, integration, and transfer to other individuals of perceived or mentally-fabricated patterns. During human evolution, pattern processing capabilities became increasingly sophisticated as the result of expansion of the cerebral cortex, particularly the prefrontal cortex and regions involved in processing of images. Specific patterns, real or imagined, are reinforced by emotional experiences, indoctrination and even psychedelic drugs. Impaired or dysregulated SPP is fundamental to cognitive and psychiatric disorders. A broader understanding of SPP mechanisms, and their roles in normal and abnormal function of the human brain, may enable the development of interventions that reduce irrational decisions and destructive behaviors.}
}

@article{belenguer_ai_2022,
	title = {{AI} bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry},
	volume = {2},
	issn = {2730-5953},
	shorttitle = {{AI} bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8830968/},
	doi = {10.1007/s43681-022-00138-8},
	abstract = {A new and unorthodox approach to deal with discriminatory bias in Artificial Intelligence is needed. As it is explored in detail, the current literature is a dichotomy with studies originating from the contrasting fields of study of either philosophy and sociology or data science and programming. It is suggested that there is a need instead for an integration of both academic approaches, and needs to be machine-centric rather than human-centric applied with a deep understanding of societal and individual prejudices. This article is a novel approach developed into a framework of action: a bias impact assessment to raise awareness of bias and why, a clear set of methodologies as shown in a table comparing with the four stages of pharmaceutical trials, and a summary flowchart. Finally, this study concludes the need for a transnational independent body with enough power to guarantee the implementation of those solutions.},
	number = {4},
	urldate = {2023-05-02},
	journal = {Ai and Ethics},
	author = {Belenguer, Lorenzo},
	year = {2022},
	pmid = {35194591},
	pmcid = {PMC8830968},
	pages = {771--787}
}

@misc{AI_paternalism,
	title = {Artificial intelligence is infiltrating health care. {We} shouldn’t let it make all the decisions.},
	url = {https://www.technologyreview.com/2023/04/21/1071921/ai-is-infiltrating-health-care-we-shouldnt-let-it-make-decisions/},
	abstract = {AI paternalism could put patient autonomy at risk—if we let it.},
	language = {en},
	author = {Jessica Hamzelou},
	urldate = {2023-05-08},
	journal = {MIT Technology Review},
}


@misc{kowaleski_can_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Can {Ethics} be {Taught}? {Evidence} from {Securities} {Exams} and {Investment} {Adviser} {Misconduct}},
	shorttitle = {Can {Ethics} be {Taught}?},
	url = {https://papers.ssrn.com/abstract=3457588},
	doi = {10.2139/ssrn.3457588},
	abstract = {We study the consequences of a 2010 change in the investment adviser qualification exam that reallocated coverage from the rules and ethics section to the technical material section. Comparing advisers with the same employer in the same location and year, we find those passing the exam with more rules and ethics coverage are one-fourth less likely to commit misconduct. The exam change appears to affect advisers’ perception of acceptable conduct, and not just their awareness of specific rules or selection into the qualification. Those passing the rules and ethics-focused exam are more likely to depart employers experiencing scandals. Such departures also predict future scandals. Our paper offers the first archival evidence on how rules and ethics training affects conduct and labor market activity in the financial sector.},
	language = {en},
	urldate = {2023-12-12},
	author = {Kowaleski, Zachary T. and Sutherland, Andrew and Vetter, Felix},
	month = sep,
	year = {2019},
	keywords = {compliance training, ethics, ethics training, financial misconduct, financial regulation, fraud, investment advisers, labor economics},
}

@article{giorgini_researcher_2015,
	title = {Researcher {Perceptions} of {Ethical} {Guidelines} and {Codes} of {Conduct}},
	volume = {22},
	issn = {0898-9621},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313573/},
	doi = {10.1080/08989621.2014.955607},
	abstract = {Ethical codes of conduct exist in almost every profession. Field-specific codes of conduct have been around for decades, each articulating specific ethical and professional guidelines. However, there has been little empirical research on researchers’ perceptions of these codes of conduct. In the present study, we interviewed faculty members in six research disciplines and identified five themes bearing on the circumstances under which they use ethical guidelines and the underlying reasons for not adhering to such guidelines. We then identify problems with the manner in which codes of conduct in academia are constructed and offer solutions for overcoming these problems.},
	number = {3},
	urldate = {2023-12-12},
	journal = {Accountability in research},
	author = {Giorgini, Vincent and Mecca, Jensen T. and Gibson, Carter and Medeiros, Kelsey and Mumford, Michael D. and Connelly, Shane and Devenport, Lynn D.},
	year = {2015},
	pmid = {25635845},
	pmcid = {PMC4313573},
	pages = {123--138},
}


@article{pethig_biased_2023,
	title = {Biased {Humans}, ({Un}){Biased} {Algorithms}?},
	volume = {183},
	issn = {1573-0697},
	url = {https://doi.org/10.1007/s10551-022-05071-8},
	doi = {10.1007/s10551-022-05071-8},
	abstract = {Previous research has shown that algorithmic decisions can reflect gender bias. The increasingly widespread utilization of algorithms in critical decision-making domains (e.g., healthcare or hiring) can thus lead to broad and structural disadvantages for women. However, women often experience bias and discrimination through human decisions and may turn to algorithms in the hope of receiving neutral and objective evaluations. Across three studies (N = 1107), we examine whether women’s receptivity to algorithms is affected by situations in which they believe that their gender identity might disadvantage them in an evaluation process. In Study 1, we establish, in an incentive-compatible online setting, that unemployed women are more likely to choose to have their employment chances evaluated by an algorithm if the alternative is an evaluation by a man rather than a woman. Study 2 generalizes this effect by placing it in a hypothetical hiring context, and Study 3 proposes that relative algorithmic objectivity, i.e., the perceived objectivity of an algorithmic evaluator over and against a human evaluator, is a driver of women’s preferences for evaluations by algorithms as opposed to men. Our work sheds light on how women make sense of algorithms in stereotype-relevant domains and exemplifies the need to provide education for those at risk of being adversely affected by algorithmic decisions. Our results have implications for the ethical management of algorithms in evaluation settings. We advocate for improving algorithmic literacy so that evaluators and evaluatees (e.g., hiring managers and job applicants) can acquire the abilities required to reflect critically on algorithmic decisions.},
	language = {en},
	number = {3},
	urldate = {2023-12-13},
	journal = {Journal of Business Ethics},
	author = {Pethig, Florian and Kroenung, Julia},
	month = mar,
	year = {2023},
	keywords = {Algorithms, Gender bias, Objectivity, Stigma},
	pages = {637--652},
}


@article{dastin_insight_2018,
	chapter = {World},
	title = {Insight - {Amazon} scraps secret {AI} recruiting tool that showed bias against women},
	url = {https://www.reuters.com/article/idUSKCN1MK0AG/},
	abstract = {Amazon.com Inc's \&lt;AMZN.O\&gt; machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.},
	language = {en-US},
	urldate = {2023-12-13},
	journal = {Reuters},
	author = {Dastin, Jeffrey},
	month = oct,
	year = {2018},

}


@article{gichoya_ai_2022,
	title = {{AI} recognition of patient race in medical imaging: a modelling study},
	volume = {4},
	issn = {2589-7500},
	shorttitle = {{AI} recognition of patient race in medical imaging},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext},
	doi = {10.1016/S2589-7500(22)00063-2},
	language = {English},
	number = {6},
	urldate = {2023-12-13},
	journal = {The Lancet Digital Health},
	author = {Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L. and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and Kuo, Po-Chih and Lungren, Matthew P. and Palmer, Lyle J. and Price, Brandon J. and Purkayastha, Saptarshi and Pyrros, Ayis T. and Oakden-Rayner, Lauren and Okechukwu, Chima and Seyyed-Kalantari, Laleh and Trivedi, Hari and Wang, Ryan and Zaiman, Zachary and Zhang, Haoran},
	month = jun,
	year = {2022},
	pmid = {35568690},
	note = {Publisher: Elsevier},
	pages = {e406--e414},
}


@article{seyyed-kalantari_underdiagnosis_2021,
	title = {Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations},
	volume = {27},
	copyright = {2021 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01595-0},
	doi = {10.1038/s41591-021-01595-0},
	abstract = {Artificial intelligence (AI) systems have increasingly achieved expert-level performance in medical imaging applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations such as female patients, Black patients, or patients of low socioeconomic status. Such biases are especially troubling in the context of underdiagnosis, whereby the AI algorithm would inaccurately label an individual with a disease as healthy, potentially delaying access to care. Here, we examine algorithmic underdiagnosis in chest X-ray pathology classification across three large chest X-ray datasets, as well as one multi-source dataset. We find that classifiers produced using state-of-the-art computer vision techniques consistently and selectively underdiagnosed under-served patient populations and that the underdiagnosis rate was higher for intersectional under-served subpopulations, for example, Hispanic female patients. Deployment of AI systems using medical imaging for disease diagnosis with such biases risks exacerbation of existing care biases and can potentially lead to unequal access to medical treatment, thereby raising ethical concerns for the use of these models in the clinic.},
	language = {en},
	number = {12},
	urldate = {2023-12-13},
	journal = {Nature Medicine},
	author = {Seyyed-Kalantari, Laleh and Zhang, Haoran and McDermott, Matthew B. A. and Chen, Irene Y. and Ghassemi, Marzyeh},
	month = dec,
	year = {2021},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical imaging},
	pages = {2176--2182},
}


@article{ricci_lara_addressing_2022,
	title = {Addressing fairness in artificial intelligence for medical imaging},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32186-3},
	doi = {10.1038/s41467-022-32186-3},
	abstract = {A plethora of work has shown that AI systems can systematically and unfairly be biased against certain populations in multiple scenarios. The field of medical imaging, where AI systems are beginning to be increasingly adopted, is no exception. Here we discuss the meaning of fairness in this area and comment on the potential sources of biases, as well as the strategies available to mitigate them. Finally, we analyze the current state of the field, identifying strengths and highlighting areas of vacancy, challenges and opportunities that lie ahead.},
	language = {en},
	number = {1},
	urldate = {2023-12-13},
	journal = {Nature Communications},
	author = {Ricci Lara, María Agustina and Echeveste, Rodrigo and Ferrante, Enzo},
	month = aug,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning},
	pages = {4581},
}


@misc{puyol-anton_fairness_2021,
	title = {Fairness in {Cardiac} {MR} {Image} {Analysis}: {An} {Investigation} of {Bias} {Due} to {Data} {Imbalance} in {Deep} {Learning} {Based} {Segmentation}},
	shorttitle = {Fairness in {Cardiac} {MR} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2106.12387},
	doi = {10.48550/arXiv.2106.12387},
	abstract = {The subject of "fairness" in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most applications to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we perform such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice performance between different racial groups. To reduce the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly optimized with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies improved fairness, with the best SD and SER resulting from the use of protected group models.},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Puyol-Anton, Esther and Ruijsink, Bram and Piechnik, Stefan K. and Neubauer, Stefan and Petersen, Steffen E. and Razavi, Reza and King, Andrew P.},
	month = jul,
	year = {2021},
	note = {arXiv:2106.12387 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: MICCAI 2021 conference},
	file = {arXiv Fulltext PDF:/Users/carriewright/Zotero/storage/X9LN5YTJ/Puyol-Anton et al. - 2021 - Fairness in Cardiac MR Image Analysis An Investig.pdf:application/pdf;arXiv.org Snapshot:/Users/carriewright/Zotero/storage/6EN5T3JE/2106.html:text/html},
}


@article{pierson_algorithmic_2021,
	title = {An algorithmic approach to reducing unexplained pain disparities in underserved populations},
	volume = {27},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-020-01192-7},
	doi = {10.1038/s41591-020-01192-7},
	abstract = {Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Here we use a deep learning approach to measure the severity of osteoarthritis, by using knee X-rays to predict patients’ experienced pain. We show that this approach dramatically reduces unexplained racial disparities in pain. Relative to standard measures of severity graded by radiologists, which accounted for only 9\% (95\% confidence interval (CI), 3–16\%) of racial disparities in pain, algorithmic predictions accounted for 43\% of disparities, or 4.7× more (95\% CI, 3.2–11.8×), with similar results for lower-income and less-educated patients. This suggests that much of underserved patients’ pain stems from factors within the knee not reflected in standard radiographic measures of severity. We show that the algorithm’s ability to reduce unexplained disparities is rooted in the racial and socioeconomic diversity of the training set. Because algorithmic severity measures better capture underserved patients’ pain, and severity measures influence treatment decisions, algorithmic predictions could potentially redress disparities in access to treatments like arthroplasty.},
	language = {en},
	number = {1},
	urldate = {2023-12-13},
	journal = {Nature Medicine},
	author = {Pierson, Emma and Cutler, David M. and Leskovec, Jure and Mullainathan, Sendhil and Obermeyer, Ziad},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Social sciences},
	pages = {136--140},
}


@misc{Arnold_23,
	title = {How {Biased} {Data} and {Algorithms} {Can} {Harm} {Health} {\textbar} {Hopkins} {Bloomberg} {Public} {Health} {Magazine}},
	url = {https://magazine.jhsph.edu/2022/how-biased-data-and-algorithms-can-harm-health},
	author =  {Arnold, Carrie},
	abstract = {Public health researchers are working to uncover and correct unfairness in AI.},
	language = {en},
	urldate = {2023-12-13},
	year= {2022}
}

@misc{Cote2022,
	title = {7 DATA COLLECTION METHODS IN BUSINESS ANALYTICS},
	url = {https://online.hbs.edu/blog/post/data-collection-methods},
	author =  {Cote, Catherine},]
	language = {en},
	urldate = {2023-12-13},
	year= {2022}
}

@misc{Walsh2023,
	title = {The legal issues presented by generative AI},
	url = {https://mitsloan.mit.edu/ideas-made-to-matter/legal-issues-presented-generative-ai},
	author =  {Walsh, Dylan},]
	language = {en},
	urldate = {2023-12-13},
	year= {2023}
}

