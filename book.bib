@online{omfif2023,
  author = {{OMFIF}},
  title = {How Central Banks Are Already Deploying Artificial Intelligence},
  year = {2023},
  month = {September},
  url = {https://www.omfif.org/2023/09/how-central-banks-are-already-deploying-artificial-intelligence/},
}

@techreport{stlouisfed2023,
  author = {{Federal Reserve Bank of St. Louis}},
  title = {Monetary Policy and Economic Performance since the Global Financial Crisis},
  institution = {Working Paper Series},
  number = {2023-015},
  year = {2023},
  url = {https://research.stlouisfed.org/wp/more/2023-015},
}

@article{nelson2023,
  author = {Nelson, E.},
  title = {European Central Bank Moves Toward Embracing Artificial Intelligence},
  journal = {The New York Times},
  year = {2023},
  month = {September 28},
  url = {https://www.nytimes.com/2023/09/28/business/european-central-bank-artificial-intelligence.html},
}

@online{moufakkir2023,
  author = {Moufakkir, M.},
  title = {Understanding and Guiding Artificial Intelligence in Central Banking},
  howpublished = {European Central Bank Blog},
  year = {2023},
  month = {September 28},
  url = {https://www.ecb.europa.eu/press/blog/date/2023/html/ecb.blog230928~3f76d57cce.en.html},
}



@Manual{rmarkdown2021,
  title = {rmarkdown: Dynamic Documents for R},
  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},
  year = {2021},
  note = {R package version 2.10},
  url = {https://github.com/rstudio/rmarkdown},
}

@Book{Xie2018,
  title = {R Markdown: The Definitive Guide},
  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2018},
  note = {ISBN 9781138359338},
  url = {https://bookdown.org/yihui/rmarkdown},
}

@Book{Xie2020,
  title = {R Markdown Cookbook},
  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2020},
  note = {ISBN 9780367563837},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook},
}

@ARTICLE{Mattson2014,
  	AUTHOR={Mattson, Mark P.},   
	 TITLE={Superior pattern processing is the essence of the evolved human brain},      
	JOURNAL={Frontiers in Neuroscience},      
	VOLUME={8},           
	YEAR={2014},      
	  URL={https://www.frontiersin.org/articles/10.3389/fnins.2014.00265},       
	DOI={10.3389/fnins.2014.00265},      
	ISSN={1662-453X},   
	ABSTRACT={Humans have long pondered the nature of their mind/brain and, particularly why its capacities for reasoning, communication and abstract thought are far superior to other species, including closely related anthropoids. This article considers superior pattern processing (SPP) as the fundamental basis of most, if not all, unique features of the human brain including intelligence, language, imagination, invention, and the belief in imaginary entities such as ghosts and gods. SPP involves the electrochemical, neuronal network-based, encoding, integration, and transfer to other individuals of perceived or mentally-fabricated patterns. During human evolution, pattern processing capabilities became increasingly sophisticated as the result of expansion of the cerebral cortex, particularly the prefrontal cortex and regions involved in processing of images. Specific patterns, real or imagined, are reinforced by emotional experiences, indoctrination and even psychedelic drugs. Impaired or dysregulated SPP is fundamental to cognitive and psychiatric disorders. A broader understanding of SPP mechanisms, and their roles in normal and abnormal function of the human brain, may enable the development of interventions that reduce irrational decisions and destructive behaviors.}
}

@article{belenguer_ai_2022,
	title = {{AI} bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry},
	volume = {2},
	issn = {2730-5953},
	shorttitle = {{AI} bias},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8830968/},
	doi = {10.1007/s43681-022-00138-8},
	abstract = {A new and unorthodox approach to deal with discriminatory bias in Artificial Intelligence is needed. As it is explored in detail, the current literature is a dichotomy with studies originating from the contrasting fields of study of either philosophy and sociology or data science and programming. It is suggested that there is a need instead for an integration of both academic approaches, and needs to be machine-centric rather than human-centric applied with a deep understanding of societal and individual prejudices. This article is a novel approach developed into a framework of action: a bias impact assessment to raise awareness of bias and why, a clear set of methodologies as shown in a table comparing with the four stages of pharmaceutical trials, and a summary flowchart. Finally, this study concludes the need for a transnational independent body with enough power to guarantee the implementation of those solutions.},
	number = {4},
	urldate = {2023-05-02},
	journal = {Ai and Ethics},
	author = {Belenguer, Lorenzo},
	year = {2022},
	pmid = {35194591},
	pmcid = {PMC8830968},
	pages = {771--787}
}

@misc{AI_paternalism,
	title = {Artificial intelligence is infiltrating health care. {We} shouldn’t let it make all the decisions.},
	url = {https://www.technologyreview.com/2023/04/21/1071921/ai-is-infiltrating-health-care-we-shouldnt-let-it-make-decisions/},
	abstract = {AI paternalism could put patient autonomy at risk—if we let it.},
	language = {en},
	author = {Jessica Hamzelou},
	urldate = {2023-05-08},
	journal = {MIT Technology Review},
}


@misc{kowaleski_can_2019,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Can {Ethics} be {Taught}? {Evidence} from {Securities} {Exams} and {Investment} {Adviser} {Misconduct}},
	shorttitle = {Can {Ethics} be {Taught}?},
	url = {https://papers.ssrn.com/abstract=3457588},
	doi = {10.2139/ssrn.3457588},
	abstract = {We study the consequences of a 2010 change in the investment adviser qualification exam that reallocated coverage from the rules and ethics section to the technical material section. Comparing advisers with the same employer in the same location and year, we find those passing the exam with more rules and ethics coverage are one-fourth less likely to commit misconduct. The exam change appears to affect advisers’ perception of acceptable conduct, and not just their awareness of specific rules or selection into the qualification. Those passing the rules and ethics-focused exam are more likely to depart employers experiencing scandals. Such departures also predict future scandals. Our paper offers the first archival evidence on how rules and ethics training affects conduct and labor market activity in the financial sector.},
	language = {en},
	urldate = {2023-12-12},
	author = {Kowaleski, Zachary T. and Sutherland, Andrew and Vetter, Felix},
	month = sep,
	year = {2019},
	keywords = {compliance training, ethics, ethics training, financial misconduct, financial regulation, fraud, investment advisers, labor economics},
}

@article{giorgini_researcher_2015,
	title = {Researcher {Perceptions} of {Ethical} {Guidelines} and {Codes} of {Conduct}},
	volume = {22},
	issn = {0898-9621},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313573/},
	doi = {10.1080/08989621.2014.955607},
	abstract = {Ethical codes of conduct exist in almost every profession. Field-specific codes of conduct have been around for decades, each articulating specific ethical and professional guidelines. However, there has been little empirical research on researchers’ perceptions of these codes of conduct. In the present study, we interviewed faculty members in six research disciplines and identified five themes bearing on the circumstances under which they use ethical guidelines and the underlying reasons for not adhering to such guidelines. We then identify problems with the manner in which codes of conduct in academia are constructed and offer solutions for overcoming these problems.},
	number = {3},
	urldate = {2023-12-12},
	journal = {Accountability in research},
	author = {Giorgini, Vincent and Mecca, Jensen T. and Gibson, Carter and Medeiros, Kelsey and Mumford, Michael D. and Connelly, Shane and Devenport, Lynn D.},
	year = {2015},
	pmid = {25635845},
	pmcid = {PMC4313573},
	pages = {123--138},
}


@article{pethig_biased_2023,
	title = {Biased {Humans}, ({Un}){Biased} {Algorithms}?},
	volume = {183},
	issn = {1573-0697},
	url = {https://doi.org/10.1007/s10551-022-05071-8},
	doi = {10.1007/s10551-022-05071-8},
	abstract = {Previous research has shown that algorithmic decisions can reflect gender bias. The increasingly widespread utilization of algorithms in critical decision-making domains (e.g., healthcare or hiring) can thus lead to broad and structural disadvantages for women. However, women often experience bias and discrimination through human decisions and may turn to algorithms in the hope of receiving neutral and objective evaluations. Across three studies (N = 1107), we examine whether women’s receptivity to algorithms is affected by situations in which they believe that their gender identity might disadvantage them in an evaluation process. In Study 1, we establish, in an incentive-compatible online setting, that unemployed women are more likely to choose to have their employment chances evaluated by an algorithm if the alternative is an evaluation by a man rather than a woman. Study 2 generalizes this effect by placing it in a hypothetical hiring context, and Study 3 proposes that relative algorithmic objectivity, i.e., the perceived objectivity of an algorithmic evaluator over and against a human evaluator, is a driver of women’s preferences for evaluations by algorithms as opposed to men. Our work sheds light on how women make sense of algorithms in stereotype-relevant domains and exemplifies the need to provide education for those at risk of being adversely affected by algorithmic decisions. Our results have implications for the ethical management of algorithms in evaluation settings. We advocate for improving algorithmic literacy so that evaluators and evaluatees (e.g., hiring managers and job applicants) can acquire the abilities required to reflect critically on algorithmic decisions.},
	language = {en},
	number = {3},
	urldate = {2023-12-13},
	journal = {Journal of Business Ethics},
	author = {Pethig, Florian and Kroenung, Julia},
	month = mar,
	year = {2023},
	keywords = {Algorithms, Gender bias, Objectivity, Stigma},
	pages = {637--652},
}


@article{dastin_insight_2018,
	chapter = {World},
	title = {Insight - {Amazon} scraps secret {AI} recruiting tool that showed bias against women},
	url = {https://www.reuters.com/article/idUSKCN1MK0AG/},
	abstract = {Amazon.com Inc's \&lt;AMZN.O\&gt; machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.},
	language = {en-US},
	urldate = {2023-12-13},
	journal = {Reuters},
	author = {Dastin, Jeffrey},
	month = oct,
	year = {2018},

}


@article{gichoya_ai_2022,
	title = {{AI} recognition of patient race in medical imaging: a modelling study},
	volume = {4},
	issn = {2589-7500},
	shorttitle = {{AI} recognition of patient race in medical imaging},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00063-2/fulltext},
	doi = {10.1016/S2589-7500(22)00063-2},
	language = {English},
	number = {6},
	urldate = {2023-12-13},
	journal = {The Lancet Digital Health},
	author = {Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L. and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and Kuo, Po-Chih and Lungren, Matthew P. and Palmer, Lyle J. and Price, Brandon J. and Purkayastha, Saptarshi and Pyrros, Ayis T. and Oakden-Rayner, Lauren and Okechukwu, Chima and Seyyed-Kalantari, Laleh and Trivedi, Hari and Wang, Ryan and Zaiman, Zachary and Zhang, Haoran},
	month = jun,
	year = {2022},
	pmid = {35568690},
	note = {Publisher: Elsevier},
	pages = {e406--e414},
}


@article{seyyed-kalantari_underdiagnosis_2021,
	title = {Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations},
	volume = {27},
	copyright = {2021 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01595-0},
	doi = {10.1038/s41591-021-01595-0},
	abstract = {Artificial intelligence (AI) systems have increasingly achieved expert-level performance in medical imaging applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations such as female patients, Black patients, or patients of low socioeconomic status. Such biases are especially troubling in the context of underdiagnosis, whereby the AI algorithm would inaccurately label an individual with a disease as healthy, potentially delaying access to care. Here, we examine algorithmic underdiagnosis in chest X-ray pathology classification across three large chest X-ray datasets, as well as one multi-source dataset. We find that classifiers produced using state-of-the-art computer vision techniques consistently and selectively underdiagnosed under-served patient populations and that the underdiagnosis rate was higher for intersectional under-served subpopulations, for example, Hispanic female patients. Deployment of AI systems using medical imaging for disease diagnosis with such biases risks exacerbation of existing care biases and can potentially lead to unequal access to medical treatment, thereby raising ethical concerns for the use of these models in the clinic.},
	language = {en},
	number = {12},
	urldate = {2023-12-13},
	journal = {Nature Medicine},
	author = {Seyyed-Kalantari, Laleh and Zhang, Haoran and McDermott, Matthew B. A. and Chen, Irene Y. and Ghassemi, Marzyeh},
	month = dec,
	year = {2021},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical imaging},
	pages = {2176--2182},
}


@article{ricci_lara_addressing_2022,
	title = {Addressing fairness in artificial intelligence for medical imaging},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32186-3},
	doi = {10.1038/s41467-022-32186-3},
	abstract = {A plethora of work has shown that AI systems can systematically and unfairly be biased against certain populations in multiple scenarios. The field of medical imaging, where AI systems are beginning to be increasingly adopted, is no exception. Here we discuss the meaning of fairness in this area and comment on the potential sources of biases, as well as the strategies available to mitigate them. Finally, we analyze the current state of the field, identifying strengths and highlighting areas of vacancy, challenges and opportunities that lie ahead.},
	language = {en},
	number = {1},
	urldate = {2023-12-13},
	journal = {Nature Communications},
	author = {Ricci Lara, María Agustina and Echeveste, Rodrigo and Ferrante, Enzo},
	month = aug,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning},
	pages = {4581},
}


@misc{puyol-anton_fairness_2021,
	title = {Fairness in {Cardiac} {MR} {Image} {Analysis}: {An} {Investigation} of {Bias} {Due} to {Data} {Imbalance} in {Deep} {Learning} {Based} {Segmentation}},
	shorttitle = {Fairness in {Cardiac} {MR} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2106.12387},
	doi = {10.48550/arXiv.2106.12387},
	abstract = {The subject of "fairness" in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most applications to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we perform such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice performance between different racial groups. To reduce the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly optimized with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies improved fairness, with the best SD and SER resulting from the use of protected group models.},
	urldate = {2023-12-13},
	publisher = {arXiv},
	author = {Puyol-Anton, Esther and Ruijsink, Bram and Piechnik, Stefan K. and Neubauer, Stefan and Petersen, Steffen E. and Razavi, Reza and King, Andrew P.},
	month = jul,
	year = {2021},
	note = {arXiv:2106.12387 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: MICCAI 2021 conference},
	file = {arXiv Fulltext PDF:/Users/carriewright/Zotero/storage/X9LN5YTJ/Puyol-Anton et al. - 2021 - Fairness in Cardiac MR Image Analysis An Investig.pdf:application/pdf;arXiv.org Snapshot:/Users/carriewright/Zotero/storage/6EN5T3JE/2106.html:text/html},
}


@article{pierson_algorithmic_2021,
	title = {An algorithmic approach to reducing unexplained pain disparities in underserved populations},
	volume = {27},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-020-01192-7},
	doi = {10.1038/s41591-020-01192-7},
	abstract = {Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Here we use a deep learning approach to measure the severity of osteoarthritis, by using knee X-rays to predict patients’ experienced pain. We show that this approach dramatically reduces unexplained racial disparities in pain. Relative to standard measures of severity graded by radiologists, which accounted for only 9\% (95\% confidence interval (CI), 3–16\%) of racial disparities in pain, algorithmic predictions accounted for 43\% of disparities, or 4.7× more (95\% CI, 3.2–11.8×), with similar results for lower-income and less-educated patients. This suggests that much of underserved patients’ pain stems from factors within the knee not reflected in standard radiographic measures of severity. We show that the algorithm’s ability to reduce unexplained disparities is rooted in the racial and socioeconomic diversity of the training set. Because algorithmic severity measures better capture underserved patients’ pain, and severity measures influence treatment decisions, algorithmic predictions could potentially redress disparities in access to treatments like arthroplasty.},
	language = {en},
	number = {1},
	urldate = {2023-12-13},
	journal = {Nature Medicine},
	author = {Pierson, Emma and Cutler, David M. and Leskovec, Jure and Mullainathan, Sendhil and Obermeyer, Ziad},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Social sciences},
	pages = {136--140},
}


@misc{Arnold_23,
	title = {How {Biased} {Data} and {Algorithms} {Can} {Harm} {Health} {\textbar} {Hopkins} {Bloomberg} {Public} {Health} {Magazine}},
	url = {https://magazine.jhsph.edu/2022/how-biased-data-and-algorithms-can-harm-health},
	author =  {Arnold, Carrie},
	abstract = {Public health researchers are working to uncover and correct unfairness in AI.},
	language = {en},
	urldate = {2023-12-13},
	year= {2022}
}

@misc{Cote2022,
	title = {7 DATA COLLECTION METHODS IN BUSINESS ANALYTICS},
	url = {https://online.hbs.edu/blog/post/data-collection-methods},
	author =  {Cote, Catherine},
	language = {en},
	urldate = {2023-12-13},
	year= {2022}
}

@misc{Walsh2023,
	title = {The legal issues presented by generative AI},
	url = {https://mitsloan.mit.edu/ideas-made-to-matter/legal-issues-presented-generative-ai},
	author =  {Walsh, Dylan},
	language = {en},
	urldate = {2023-12-13},
	year= {2023}
}

@misc{nikulski_toxicity_2021,
	title = {Toxicity in {AI} {Text} {Generation}},
	url = {https://towardsdatascience.com/toxicity-in-ai-text-generation-9e9d9646e68f},
	abstract = {This article provides an overview of toxic language generation, what toxicity in text generation means, why it occurs, and how it is currently being addressed.},
	language = {en},
	urldate = {2023-12-13},
	journal = {Medium},
	author = {Nikulski, Julia},
	month = sep,
	year = {2021},
}

@article{paul_safeguards_2020,
	title = {Safeguards for the use of artificial intelligence and machine learning in global health},
	volume = {98},
	issn = {0042-9686},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7133486/},
	doi = {10.2471/BLT.19.237099},
	number = {4},
	urldate = {2023-12-14},
	journal = {Bulletin of the World Health Organization},
	author = {Paul, Amy K and Schaefer, Merrick},
	month = apr,
	year = {2020},
	pmid = {32284653},
	pmcid = {PMC7133486},
	pages = {282--284},

}


@misc{pearce_beware_2021,
	title = {Beware the {Privacy} {Violations} in {Artificial} {Intelligence} {Applications}},
	url = {https://www.isaca.org/resources/news-and-trends/isaca-now-blog/2021/beware-the-privacy-violations-in-artificial-intelligence-applications},
	abstract = {It has been proposed that, “Privacy matters to the electorate, and smart business looks at how to use data to find out information while remaining in compliance with regulatory rules.” Since “smart business” also consists of “the electorate” as employees, at least one...},
	urldate = {2023-12-14},
	journal = {ISACA},
	author = {Pearce, Guy},
	year = {2021}
}


@misc{nigro_ai_nodate,
	title = {{AI} security risks: {Separating} hype from reality {\textbar} {Security} {Magazine}},
	shorttitle = {{AI} security risks},
	url = {https://www.securitymagazine.com/articles/100219-ai-security-risks-separating-hype-from-reality},
	abstract = {By investing in artificial intelligence training and the necessary tools, security professionals can harness the power of AI to enhance their capabilities.},
	language = {en},
	urldate = {2023-12-14},
	year = {2023},
	author = {Nigro, Pam},
}

@misc{CDC2023,
	title = {Melanoma of the Skin Statistics},
	url = {https://www.cdc.gov/cancer/skin/statistics/index.htm},
	author =  {CDC},
	language = {en},
	urldate = {2023-12-14},
	year= {2023}
}

@article{Melarkode2023,
  title={AI-Powered Diagnosis of Skin Cancer: A Contemporary Review, Open Challenges and Future Research Directions},
  author={Melarkode, Navneet and Srinivasan, Kathiravan and Qaisar, Saeed Mian and Plawiak, Pawel},
  journal={Cancers},
  volume={15},
  number={4},
  pages={1183},
  year={2023},
  publisher={MDPI}

}

{lohr_what_2021,
	title = {What {Ever} {Happened} to {IBM}’s {Watson}? - {The} {New} {York} {Times}},
	url = {https://www.nytimes.com/2021/07/16/technology/what-happened-ibm-watson.html},
	urldate = {2023-12-15},
	author = {Lohr, Steve},
	year = {2021},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? },
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2023-12-14},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{sinz_engineering_2019,
	title = {Engineering a {Less} {Artificial} {Intelligence}},
	volume = {103},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319307408},
	doi = {10.1016/j.neuron.2019.08.034},
	language = {en},
	number = {6},
	urldate = {2023-12-15},
	journal = {Neuron},
	author = {Sinz, Fabian H. and Pitkow, Xaq and Reimer, Jacob and Bethge, Matthias and Tolias, Andreas S.},
	month = sep,
	year = {2019},
	pages = {967--979},
}


@phdthesis{chaaya_privacy_2021,
	type = {phdthesis},
	title = {Privacy management in connected environments},
	url = {https://theses.hal.science/tel-03446023},
	abstract = {Recent years have witnessed rapid progress in enabling technologies for data sensing, communication and mining, paving the way for the phenomenal growth of smart connected environments (e.g., smart buildings, cities, factories). These environments are currently providing interesting and useful applications that help users in their everyday tasks (e.g. increasing comfort, reducing energy consumption). However, such applications require to collect, exchange, store, and process large amount of fine-granular data that is often privacy-sensitive for their users (e.g., location, energy-consumption), as its analysis allows data consumers to reveal sensitive information about them, such as their health conditions and preferences.Consequently, involving users in the management of their privacy is nowadays receiving extensive attention. Nonetheless, various improvements are still required. For instance, how to raise user awareness of the privacy risks involved in their data sharing and/or imposed by their environments. Moreover, how to enable users to assess their situations and make optimal data utility-privacy decisions accordingly.In this thesis, we focus on six main challenges: (1) representing diverse user contexts with a high semantic expressiveness power; (2) performing a holistic (all-data-inclusive) context-based privacy risk reasoning; (3) achieving user-centric privacy management; (4) making optimal context-based privacy decisions; (5) coping with the inter-context data dependency; and (6) delivering scalability and efficiency in order to assist the user in a variety of situations.To address these challenges, we first present an ontology-based data model capable of representing various user contexts with high-level information coverage. Following that, we introduce a context-aware semantic reasoning approach for privacy risk inference that provides a dynamic/contextual overview of risks tailored to the user's expertise. Then, to enable optimal management of data utility-privacy trade-offs, we propose a user-centric multi-objective approach for context-aware privacy management that provides dynamic best data protection strategies to be implemented based on user situations and preferences. Finally, we propose a new stochastic gradient descent solution for privacy-preserving during protection transitions, which gives an additional layer of protection against data inference attacks.The aforementioned contributions are regrouped in one global generic and extensible framework for context-aware privacy management.},
	language = {en},
	urldate = {2023-12-15},
	school = {Université de Pau et des Pays de l'Adour},
	author = {Chaaya, Karam Bou},
	month = sep,
	year = {2021},

}

@article{tucker_privacy_2018,
	title = {Privacy, {Algorithms}, and {Artificial} {Intelligence}},
	url = {https://ideas.repec.org//h/nbr/nberch/14011.html},
	abstract = {No abstract is available for this item.},
	language = {en},
	urldate = {2023-12-15},
	journal = {NBER Chapters},
	author = {Tucker, Catherine},
	year = {2018},
	note = {Publisher: National Bureau of Economic Research, Inc},
	pages = {423--437},
}

@misc{elefant_can_2023,
	title = {Can {Lawyers} {Legally} and {Ethically} {Record} {Conversations} {With} {Clients} {Using} {Artificial} {Intelligence}?},
	url = {https://myshingle.com/2023/07/articles/artificial-intelligence/can-lawyers-legally-and-ethically-record-conversations-with-clients-using-artificial-intelligence/},
	abstract = {Abstract (prepared with ChatGPT 4): The rise of artificial intelligence (AI) and a surge in online meetings has made it increasingly common for attorneys to record and transcribe client conversatio…},
	language = {en-US},
	urldate = {2023-12-15},
	journal = {My Shingle},
	author = {Elefant, Carolyn},
	month = jul,
	year = {2023},
}


@article{andreotta_ai_2022,
	title = {{AI}, big data, and the future of consent},
	volume = {37},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-021-01262-5},
	doi = {10.1007/s00146-021-01262-5},
	abstract = {In this paper, we discuss several problems with current Big data practices which, we claim, seriously erode the role of informed consent as it pertains to the use of personal information. To illustrate these problems, we consider how the notion of informed consent has been understood and operationalised in the ethical regulation of biomedical research (and medical practices, more broadly) and compare this with current Big data practices. We do so by first discussing three types of problems that can impede informed consent with respect to Big data use. First, we discuss the transparency (or explanation) problem. Second, we discuss the re-repurposed data problem. Third, we discuss the meaningful alternatives problem. In the final section of the paper, we suggest some solutions to these problems. In particular, we propose that the use of personal data for commercial and administrative objectives could be subject to a ‘soft governance’ ethical regulation, akin to the way that all projects involving human participants (e.g., social science projects, human medical data and tissue use) are regulated in Australia through the Human Research Ethics Committees (HRECs). We also consider alternatives to the standard consent forms, and privacy policies, that could make use of some of the latest research focussed on the usability of pictorial legal contracts.},
	language = {en},
	number = {4},
	urldate = {2023-12-15},
	journal = {AI \& SOCIETY},
	author = {Andreotta, Adam J. and Kirkham, Nin and Rizzi, Marco},
	month = dec,
	year = {2022},
	keywords = {AI, Big data, Informed consent, Moral responsibility, Privacy},
	pages = {1715--1728},
}

@misc{Leek2017,
	title = {Demystifying Artificial Intelligence},
	url = {https://leanpub.com/demystifyai},
	author =  {Leek, Jeffrey T and Narayanan, Divya},
	language = {en},
	year= {2017}
}

@misc{ibm2023,
	title = {AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the difference?},
	url = {https://www.ibm.com/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks/},
	language = {en-US},
	urldate = {2023-12-15},
	author = {IBM Data \& AI Team},
	year = {2023}
}

@misc{wikiNLP,
	title = {Natural language processing},
	url = {https://en.wikipedia.org/wiki/Natural_language_processing#History},
	language = {en-US},
	urldate = {2023-12-15},
	author = {Wikipedia},
	year = {2023}
}

@misc{gangarapu_ethics_2022,
	title = {Ethics of {Facial} {Recognition}: {Key} {Issues} and {Solutions}},
	shorttitle = {Ethics of {Facial} {Recognition}},
	url = {https://learn.g2.com/ethics-of-facial-recognition},
	abstract = {Facial recognition is one of the most advanced forms of biometric security facing ethical issues. Learn more about these issues and ways to mitigate them.},
	language = {en},
	urldate = {2023-12-15},
	author = {Gangarapu, Katam Raju},
	year = {2022},
}


@article{van_noorden_ethical_2020,
	title = {The ethical questions that haunt facial-recognition research},
	volume = {587},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-020-03187-3},
	doi = {10.1038/d41586-020-03187-3},
	abstract = {Journals and researchers are under fire for controversial studies using this technology. And a Nature survey reveals that many researchers in this field think there is a problem.},
	language = {en},
	number = {7834},
	urldate = {2023-12-15},
	journal = {Nature},
	author = {Van Noorden, Richard},
	month = nov,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: News Feature
Number: 7834
Publisher: Nature Publishing Group
Subject\_term: Machine learning, Ethics, Politics, Computer science},
	keywords = {Computer science, Ethics, Machine learning, Politics},
	pages = {354--358}
}


@misc{hao_deleting_2021,
	title = {Deleting unethical data sets isn’t good enough},
	url = {https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/},
	abstract = {The AI research community has tried to scrub away its past. But the internet is forever.},
	language = {en},
	urldate = {2023-12-15},
	journal = {MIT Technology Review},
	author = {Hao, Karen},
	year = {2021},
}

@misc{Gates_principles,
	title = {The first principles guiding our work with {AI}},
	url = {https://www.gatesfoundation.org/ideas/articles/artificial-intelligence-ai-development-principles},
	abstract = {Gates Foundation CEO Mark Suzman shares the first principles on artificial intelligence (AI) that will be used to guide the foundation’s use of AI.},
	language = {en},
	urldate = {2023-12-15},
	journal = {Bill \& Melinda Gates Foundation},
	file = {Snapshot:/Users/carriewright/Zotero/storage/23P8UXSH/artificial-intelligence-ai-development-principles.html:text/html},
}

@article{fogel2022,
  title={Defining artificial intelligence},
  author={Fogel, David B},
  journal={Machine Learning and the City: Applications in Architecture and Urban Design},
  pages={91--120},
  year={2022},
  publisher={Wiley Online Library}
}

@misc{wikipedia_training_2023,
	title = {Training, validation, and test data sets},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Training,_validation,_and_test_data_sets&oldid=1188399008},
	abstract = {In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided into multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation, and test sets.
The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.
Successively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).
This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term "validation set" is sometimes used instead of "test set" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.},
	language = {en},
	urldate = {2023-12-16},
	journal = {Wikipedia},
	month = dec,
	year = {2023},
	note = {Page Version ID: 1188399008},
}


@article{baker_algorithmic_2022,
	title = {Algorithmic {Bias} in {Education}},
	volume = {32},
	issn = {1560-4292},
	doi = {10.1007/s40593-021-00285-9},
	abstract = {In this paper, we review algorithmic bias in education, discussing the causes of that bias and reviewing the empirical literature on the specific ways that algorithmic bias is known to have manifested in education. While other recent work has reviewed mathematical definitions of fairness and expanded algorithmic approaches to reducing bias, our review focuses instead on solidifying the current understanding of the concrete impacts of algorithmic bias in education--which groups are known to be impacted and which stages and agents in the development and deployment of educational algorithms are implicated. We discuss theoretical and formal perspectives on algorithmic bias, connect those perspectives to the machine learning pipeline, and review metrics for assessing bias. Next, we review the evidence around algorithmic bias in education, beginning with the most heavily-studied categories of race/ethnicity, gender, and nationality, and moving to the available evidence of bias for less-studied categories, such as socioeconomic status, disability, and military-connected status. Acknowledging the gaps in what has been studied, we propose a framework for moving from unknown bias to known bias and from fairness to equity. We discuss obstacles to addressing these challenges and propose four areas of effort for mitigating and resolving the problems of algorithmic bias in AIED systems and other educational technology.},
	language = {en},
	number = {4},
	urldate = {2023-12-17},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Baker, Ryan S. and Hawn, Aaron},
	month = dec,
	year = {2022},
	note = {Publisher: Springer
ERIC Number: EJ1353563},
	keywords = {Artificial Intelligence, Bias, Disabilities, Education, Mathematics, Military Personnel, Race, Sex, Socioeconomic Status, Veterans},
	pages = {1052--1092},
}


@article{huang_evaluation_2022,
	title = {Evaluation and {Mitigation} of {Racial} {Bias} in {Clinical} {Machine} {Learning} {Models}: {Scoping} {Review}},
	volume = {10},
	shorttitle = {Evaluation and {Mitigation} of {Racial} {Bias} in {Clinical} {Machine} {Learning} {Models}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9198828/},
	doi = {10.2196/36388},
	abstract = {Racial bias is a key concern regarding the development, validation, and implementation of machine learning (ML) models in clinical settings. Despite the potential of bias to propagate health disparities, racial bias in clinical ML has yet to be thoroughly ...},
	language = {en},
	number = {5},
	urldate = {2023-12-17},
	journal = {JMIR Medical Informatics},
	author = {Huang, Jonathan and Galal, Galal and Etemadi, Mozziyar and Vaidyanathan, Mahesh},
	month = may,
	year = {2022},
	pmid = {35639450},
	note = {Publisher: JMIR Publications Inc.},
}

@misc{wikiECHO,
	title = {Amazon Echo},
	url = {https://en.wikipedia.org/wiki/Amazon_Echo},
	language = {en-US},
	urldate = {2023-12-17},
	author = {Wikipedia},
	year = {2023}
}
