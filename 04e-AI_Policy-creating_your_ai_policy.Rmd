```{r, include = FALSE}
ottrpal::set_knitr_image_path()
```

# Considerations for creating an AI Policy

AI tools are already changing how we work, and they will continue to do so for years. Over the next few years, we're likely going to see AI used in ways we've never imagined and are not anticipating. How can you guide your organization to adopt AI in a way that's not unethical, illegal, or wrong?

<div class = disclaimer>
`r config::get("disclaimer")`
</div>

## An AI policy alone is not enough

It is probably not enough to build only an AI policy. Building in an AI support system that make it possible for the people in your organization to adopt AI in safe and ethical ways is also important. An AI policy support system might include the infrastructure to handle AI, guidelines on AI best practices, and training materials in addition to the policy. 

Tools like LLMs are increasingly popular, and it is unlikely that your organization will be able to completely ban their use. Instead, a more effective approach that allows you to protect your organization might be to adopt a policy allowing LLM use in specific circumstances, then create a support system around that.

For example, many AIs have a user agreement. Two possible types of user agreements are commercial agreements (which is what individual users generally have) and enterprise agreements (which is what organizations and institutions might have). The terms and conditions of enterprise agreements tend to be more stable over time and can be negotiated to include terms favorable to your organization, like your organization's data not being used as part of the AI's training data. 

If an employee uses an AI system as a single consumer, they will generally sign a consumer use agreement. Under a consumer agreement, your employee may have fewer legal protections than they would if they were operating under an enterprise agreement. Consumer agreements can change unexpectedly, which means you could be operating under a whole new set of circumstances month to month. Additionally, consumers do not have the same sort of negotiating power with consumer agreements, which means a single employee is unlikely to have the same sort of data protections that an institution might have. By negotiating an enterprise agreement, your organization has created a system in which employees and their actions are less likely to result in unintential harm or data misuse.

Thinking about your AI policy as just the beginning, not the entire thing, can be a way to protect your employees, your organization, and the people you serve.

## Get lots of voices weighing in from the beginning

AI systems are being integrated into every aspect of the work environment. You probably need a lot of different people to weigh in to get even close to what you want in terms of a comprehensive AI policy. Limiting policy creation to just the Chief Data Officer's office or the IT department or the legal department might make things faster, but the trade-off is that you are likely only covering a fraction of what you need. At minimum, most organizations probably need representatives from legal, compliance and governance, IT , offices of diversity, equity, inclusion, ethical review, and training. Creating a meaningful policy and getting the necessary supports put in place is easier when you have people with varied and broad expertise creating the policy from the beginning.

## Consider how to keep your guidance agile

The speed at which AI technology is changing is fast enough that creating useful guidelines around its use is difficult. An AI policy requires you to get a diverse set of opinions together and make it cohesive and coherent, and that takes time. The last thing you want to do is create a policy that no longer applies in 3 months when AI systems have changed again.

So how can you systematize the process of doing creating your policy so that you can easily update it when necessary? One possible way is to think of your AI Policy as an ongoing living thing as opposed to a one time effort. This might include creating both an AI Policy and an AI Best Practices document. 

In a situation like this, the policy evolves more slowly and the best practices evolves more quickly. For example, the policy document might say something like "you should use infrastructure that matches current best practices." This allows you to create a policy that is still useful over time as your organization learns what AI practices and infrastructure is best for it.

The best practices can change more rapidly with more frequent updates, leaving the policy somewhat stable. This still requires you to communicate frequently with your employees on the state of the best practices for AI use. However, the best practices can be tailored to fit specific departments and change as those departments need it to do so. This also allows an organization to communicate to specific departments and employees who might be affected by an update to their best practices guidelines.

You should also consider whether asynchronous collaboration versus synchronous collaboration is right for your organization when creating and updating your policy and best practices documents. With an asynchronous approach, people write their individual sections of a document by a deadline, after which the full policy is polished and edited. With a synchronous approach, an organization might convene a set of meetings with experts over a length or time to work on the document together. There are benefits and drawbacks to both approaches, and you will know which best fits your organization's needs.

## Make it easy for people to follow your policy through effective training

Good AI policies are most effective when they are easy for people to follow. This can be particularly challenging in periods of explosive technological growth like we're experiencing now with AI. What is possible with AI, and how to safely and ethically use AI, is changing quickly, making it a challenge for people to always know how to comply with an AI policy.

In situations like these, one way to approach training is to focus on major points people should consider, clearly outline the steps people can take to do the right thing, and identify who people can approach when they have questions. Many people may not solidly know the answers to all questions, but the right people can help you find the answer. Training people how to loop in the proper people, and to ask for help from the very beginning, might save them stress later. 

# VIDEO Title!
