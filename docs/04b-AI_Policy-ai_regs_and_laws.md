

# AI acts, orders, and regulations

A good AI policy should guide an organization on AI uses that adhere to the necessary laws and regulations. With generative AI still new in many fields, from medicine to law, regulations are rapidly evolving. A landmark provisional deal on AI regulation was reached by the European Parliament and Council on December 8, 2023, with the [EU AI Act](https://artificialintelligenceact.eu/documents/)). These guidelines laid out in this document apply to AI regulation and use within the 27-member EU bloc, as well as to foreign companies that operate within the EU. It is likely the EU AI Act will serve as a model for AI laws around the globe, for both individual countries and industries.

<img src="resources/images/04b-AI_Policy-ai_regs_and_laws_files/figure-html//1PSprPB9RrNJh_01BXAcj9jn6NK2XzWx74vD_YmQyliM_g1965a5f7f0a_0_44.png" width="100%" />

Countries outside of the EU are drafting their own laws, orders, and standards surrounding AI use, so you and your employees will need to do some research on what it and is not allowed in your local area. **Always consult your legal council about the AI regulations that apply to you.**

<div class = disclaimer>
**Disclaimer:** The thoughts and ideas presented in this course are not to be substituted for legal or ethical advice and are only meant to give you a starting point for gathering information about AI policy and regulations to consider.
</div>

## The EU AI Act

According to EU policymakers involved in writing the AI Act, the goal of the Act is to regulate AI in order to limit its capacity to cause harm. The political agreement covers the use of AI in biometric surveillance (such as facial recognition), as well as guidance on regulations for LLMs. The EU AI Act divides AI-based products into levels based on how much risk each product might pose to things like data privacy and protection. Higher-risk products with a greater capacity to cause harm face more stringent rules and regulations. Most current AI uses (like systems that make recommendations) will not fall into this higher-risk category.

<img src="resources/images/04b-AI_Policy-ai_regs_and_laws_files/figure-html//1PSprPB9RrNJh_01BXAcj9jn6NK2XzWx74vD_YmQyliM_gcf1264c749_0_135.png" width="100%" />

Final details are still being worked out, but we do know several important aspects of this Act.

1. **All content generated by AI must be clearly identified.** Companies must also make it clear when customers are interacting with chatbots, emotion recognition systems, and models using biometric categorization.

1. Developers of foundational models like GPT as well as general purpose AI systems (GPAI) must **create technical documentation and detailed summaries about the training data** before they can be released on the market.

1. **High-risk AI systems** must undergo mandatory rights impact and mitigation assessments. Developers will also have to conduct model evaluations, assess and track possible cybersecurity risks, and report serious incidents and breaches to the European Commission. They will also have to use high-quality datasets and have better accompanying documentation.

1. **Open-source software** is excluded from regulations, with some exceptions for software that is considered a high-risk system or is a prohibited application.

1. AI software for manipulative strategies like deepfakes and automated disinformation campaigns, systems exploiting vulnerabilities, and indiscriminate scraping of facial images from the internet or security footage to create facial recognition databases are banned. Additional prohibited applications may be added later.

1. There are exceptions to the facial scraping ban that allow law enforcement and intelligence agencies to use AI applications for facial recognition purposes.

The AI Act also lays out financial penalties for companies that violate these regulations, which can be between 1.5% and 7% of a company's global revenue. More severe violations result in greater financial penalties.

This is still a provisional agreement. It must be approved by both the European Parliament and European countries before becoming law. After approval, tech companies will have two years to implement the rules, though bans on AI uses start six months after the EU AI Act is ratified. 

More information about the EU's AI Act can be found in these sources.

[MIT Technology Review](https://www.technologyreview.com/2023/12/11/1084942/five-things-you-need-to-know-about-the-eus-new-ai-act/)

[Reuters](https://www.reuters.com/technology/stalled-eu-ai-act-talks-set-resume-2023-12-08/)

[EURACTIV.com](https://www.euractiv.com/section/artificial-intelligence/news/ai-act-eu-policymakers-nail-down-rules-on-ai-models-butt-heads-on-law-enforcement/)

[CIO Dive](https://www.ciodive.com/news/EU-AI-Act-penalties-guardrails-foundational-models/702192/)

## Industry-specific policies

Some individual industries have already begun adopting policies about generative AI. They may also have long-standing policies in place about the use of other forms of AI, like machine learning. Some countries have also begun creating policies for specific industries and fields. When in doubt, always check with the experts within your organization about what AI policies exist for your industry.

<img src="resources/images/04b-AI_Policy-ai_regs_and_laws_files/figure-html//1PSprPB9RrNJh_01BXAcj9jn6NK2XzWx74vD_YmQyliM_g2a84ae71e54_0_67.png" width="100%" />
We'll discuss some specific examples of how different industries are approaching AI regulation in the next section.

# Case Studies

AI regulations and policies are continuing to evolve as people adapt to the use of AI. Let's look at some real-life examples.

## Education

For students and educators, generative AI's capacity in writing, problem solving, and conducting research has upended the goals and evaluations of our education system. For instance, ChatGPT 4 has been able to generate college-level essays to earn passing grades at Harvard with minimal prompting for various subjects (@slowboring). Many educational institutions reacted with various policies and adaptations; first to protect the current educational environment, then to consider adapting to generative AI's capacity. 

In the first few months after ChatGPT was released, many schools and universities restricted the use of AI in education. The two largest public school systems in the United States, New York City Public Schools and Los Angeles Public Schools, banned the use of ChatGPT in any school work, declaring that any use of ChatGPT counted as plagiarism @nytimes-technology1. Many universities also followed with similar policies. However, educators soon realized that most students embraced generative AI despite the ban for most assignments (@chronicle-higher-ed, @washingtonpost-opinions). Furthermore, enforcement to bar AI from students, such as using AI detection software or banning AI from school networks, created disparities in students. Teachers noticed that AI detection software biased against the writings of non-native English learners (@washingtonpost-opinions). Children from wealthy families could also access AI through personal smartphones or computers (@nytimes-technology1).

With these lessons, some educational systems started to embrace the role of AI in students' lives and are developing less-restrictive various policies. New York City Public School and Los Angeles Public Schools quietly rolled back their ban, as did many universities (@nytimes-technology1). Groups of educators have come together to give guidelines and resources on how to teach with the use of AI, such as the [Mississippi AI Institute](https://mississippi.ai/institute/), [MIT's Daily-AI curriculum](https://raise.mit.edu/daily/), and [Gettysburg College's Center for Creative Teaching and Learning](https://genai.sites.gettysburg.edu/). 

Each educational institution and classroom is adapting to AI differently. The Mississippi AI Institute suggested that there are some common questions to consider (@mississippi-ai-blog):

-   _How are we inviting students to demonstrate their knowledge, and is writing the only (or the best) way to do that?_ For instance, some universities have encouraged the use of in-class assignments, handwritten papers, group work and oral exams (@nytimes-technology2).

-   _What are our (new) assignment goals? And (how) might generative AI help or hinder students in reaching those goals?_ Some educators want to use AI to help students get over early brainstorming hurdles, and want students to focus on deeper critical thinking problems (@washingtonpost-opinions). Many educators have started to develop AI literacy and "critical computing" curricula to teach students how to use AI effectively and critically (@nytimes-technology3).

-   _If we’re asking students to do something that AI can do with equal facility, is it still worth asking students to do? And if so, why?_ Educators will need to think about what aspects of their lesson goals will be automated in the future, and what are critical and creative skills that students need to hone in on. 

-   _If we think students will use AI to circumvent learning, why would they want to do that? How can we create conditions that motivate students to learn for themselves?_ 
 Educators have started to teach young students the limits of AI creativity and what kind of bias is embedded in AI models, which has led students to think more critically about use of AI (@nytimes-technology3). 

-   _What structural conditions would need to change in order for AI to empower, rather than threaten, teachers and learners? How can we create those conditions?_  Some teachers have started to actively learn how their students use AI, and are using AI to assist with writing their teaching curriculum (@nytimes-technology1).


## Healthcare

The health care industry is an example of an industry where the speed of technology development has led to gaps in regulation, and the US recently released an Executive Order about creating [healthcare-specific AI policies](https://www.whitehouse.gov/briefing-room/blog/2023/12/14/delivering-on-the-promise-of-ai-to-improve-health-outcomes/). 

The U.S. Food and Drug Administration (FDA) regulates AI-enabled medical devices and software used in disease prevention, diagnosis, and treatment. However, there are serious concerns about the adequacy of current regulation, and many other AI-enabled technologies that may have clinical applications fall out of the scope of FDA regulation (@habib2023; @ama2023). Other federal agencies, such as the Health and Human Services Office of Civil Rights, have important roles in the oversight of some aspects of AI use in health care, but their authority is limited. Additionally, there are existing federal and state laws and regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), that impact the use and development of AI. This patchwork landscape of federal and state authority and existing laws has led the American Medical Association (AMA) to advocate for a “whole government” approach to implement a comprehensive set of policies to ensure that “the benefits of AI in health care are maximized while potential harms are minimized” (@healthcareradio2023).

The AMA and health care leaders have highlighted the importance of specialized expertise in the oversight and adoption of AI products in health care delivery and operations. For example, Dr. Nigam Shah and colleagues call for the medical community to take the lead in defining how LLMs are trained and developed:

>   By not asking how the intended medical use can shape the training of LLMs and the chatbots or other applications they power, technology companies are deciding what is right for medicine (@nigam2023).

The medical community should actively shape the development of AI-enabled technologies by advocating for clinically-informed standards for the training of AI, and for the evaluation of the value of AI in real-world health care settings. At an institutional level, specialized clinical expertise is required to create policies that align AI adoption with standards for health care delivery. And in-depth knowledge of U.S. health insurance system is required to understand how complexity and lack of standardization in this landscape may impact AI adoption in clinical operations (schulman2023). In summary, health care leaders and the medical community need to play an active role in the development of new AI regulations and policy.

# VIDEO AI acts, orders, and policies
